{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a4ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from minlora import (\n",
    "    LoRAParametrization,\n",
    "    add_lora,\n",
    "    apply_to_lora,\n",
    "    merge_lora,\n",
    ")\n",
    "from torch import nn\n",
    "\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2baccfbd",
   "metadata": {},
   "source": [
    "## Adding LoRA to layers other than nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec04a954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ParametrizedEmbedding(\n",
       "    3, 2\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): ParametrizedLinear(\n",
       "    in_features=2, out_features=3, bias=True\n",
       "    (parametrizations): ModuleDict(\n",
       "      (weight): ParametrizationList(\n",
       "        (0): LoRAParametrization()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## add_lora supports an optional `lora_config` argument of type Dict[Type[nn.Module], Dict[str, Callable]]\n",
    "## it specifies how to apply lora to each layer\n",
    "\n",
    "## Currently, there are support for nn.Embedding, nn.Linear, and nn.Conv2d\n",
    "\n",
    "lora_config = {\n",
    "    nn.Embedding: {\n",
    "        \"weight\": partial(LoRAParametrization.from_embedding, rank=4),\n",
    "    },\n",
    "    nn.Linear: {\n",
    "        \"weight\": partial(LoRAParametrization.from_linear, rank=4),\n",
    "    },\n",
    "}\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Embedding(num_embeddings=3, embedding_dim=2),\n",
    "    nn.Linear(in_features=2, out_features=3),\n",
    ")\n",
    "add_lora(model, lora_config=lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96024c",
   "metadata": {},
   "source": [
    "## Tying weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b649fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# let's see if this works\n",
    "linear = nn.Linear(in_features=2, out_features=3)\n",
    "embedding = nn.Embedding(num_embeddings=3, embedding_dim=2)\n",
    "# tie the weights of the linear layer and the embedding layer\n",
    "embedding.weight = linear.weight\n",
    "print(torch.allclose(embedding.weight, linear.weight))\n",
    "# so far so good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7d00069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# now, add lora to the linear layer\n",
    "add_lora(linear)\n",
    "# and update the lora weights to make it non-zero\n",
    "linear.apply(apply_to_lora(lambda x: nn.init.ones_(x.lora_B)))\n",
    "# and the weights are no longer the same\n",
    "print(torch.allclose(embedding.weight, linear.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e6e9bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'torch.nn.parameter.Parameter'>\n"
     ]
    }
   ],
   "source": [
    "# because adding lora makes the `weight` a computed property that returns a tensor.\n",
    "# It's not a Parameter anymore\n",
    "print(type(linear.weight), type(embedding.weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02d2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to tie the weights, we need to add lora to the embedding layer as well\n",
    "# let's add lora to the embedding layer\n",
    "\n",
    "add_lora(embedding, lora_config=lora_config)\n",
    "# tie the lora weights\n",
    "# because the fan_in and fan_out are opposite to each other, we need to swap the lora weights A and B\n",
    "# here we assign the linear layer's A to the embedding layer's B, and vice versa\n",
    "# you can do it the other way around as well, but the initialization will be different\n",
    "embedding.parametrizations.weight[0].lora_A = linear.parametrizations.weight[0].lora_B\n",
    "embedding.parametrizations.weight[0].lora_B = linear.parametrizations.weight[0].lora_A\n",
    "linear.apply(apply_to_lora(lambda x: nn.init.uniform_(x.lora_B)))\n",
    "linear.apply(apply_to_lora(lambda x: nn.init.uniform_(x.lora_B)))\n",
    "assert torch.allclose(linear.weight, embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f34b1f1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4]) torch.Size([4, 2]) torch.Size([4, 2]) torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# although the shape of the weight is the same, the lora parameters have different shapes\n",
    "print(\n",
    "    embedding.parametrizations.weight[0].lora_A.shape,\n",
    "    linear.parametrizations.weight[0].lora_A.shape,\n",
    "    embedding.parametrizations.weight[0].lora_B.shape,\n",
    "    linear.parametrizations.weight[0].lora_B.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ada79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# update to the linear layer will also update the embedding layer\n",
    "linear.apply(apply_to_lora(lambda x: nn.init.uniform_(x.lora_B)))\n",
    "linear.apply(apply_to_lora(lambda x: nn.init.uniform_(x.lora_A)))\n",
    "print(torch.allclose(linear.weight, embedding.weight))\n",
    "# vice versa\n",
    "embedding.apply(apply_to_lora(lambda x: nn.init.uniform_(x.lora_B)))\n",
    "embedding.apply(apply_to_lora(lambda x: nn.init.uniform_(x.lora_A)))\n",
    "print(torch.allclose(linear.weight, embedding.weight))\n",
    "# embedding.apply(apply_to_lora(lambda x: print(x.lora_B, x.lora_A)))\n",
    "# linear.apply(apply_to_lora(lambda x: print(x.lora_B, x.lora_A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51c20159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can put the logic of tying the weights in a function\n",
    "def tie_weights(linear: nn.Linear, embedding: nn.Embedding):\n",
    "    \"\"\"tie the weights of the linear layer and the embedding layer both with the same lora\"\"\"\n",
    "    # this line below is optional if the original is already tied\n",
    "    embedding.parametrizations.weight.original = linear.parametrizations.weight.original\n",
    "    embedding.parametrizations.weight[0].lora_A = linear.parametrizations.weight[0].lora_B\n",
    "    embedding.parametrizations.weight[0].lora_B = linear.parametrizations.weight[0].lora_A\n",
    "# you can import this function directly:\n",
    "from minlora import tie_weights, untie_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d5cc0b4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# now back to our first model with lora\n",
    "tie_weights(model[0], model[1])\n",
    "# update the lora weights of the linear layer\n",
    "apply_to_lora(lambda x: nn.init.uniform_(x.lora_B))(model[1])\n",
    "# and the weights are the still the same\n",
    "assert torch.allclose(model[0].weight, model[1].weight)\n",
    "merge_lora(model)\n",
    "# even after merging lora, the weights are still the same\n",
    "assert torch.allclose(model[0].weight, model[1].weight)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd38cab5b092fbce1866c43acaed152c77b80a12cd5e2b7fb23112c1a171e061"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
